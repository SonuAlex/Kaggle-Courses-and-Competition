{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f70554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "UNITS = 2 ** 11 # 2048\n",
    "ACTIVATION = 'relu'\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d069c045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.12.0\n",
      "Number of accelerators:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\instr\\AppData\\Local\\Temp\\ipykernel_33932\\2740814005.py:17: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version \" + tf.__version__)\n",
    "\n",
    "# Detect and init the TPU\n",
    "try: # detect TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() #TPU detection\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPU\n",
    "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large', \n",
    "      titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n",
    "# Data\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.io import FixedLenFeature\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c295f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(feature_description):\n",
    "    def decoder(example):\n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        features = tf.io.parse_tensor(example['features'], tf.float32)\n",
    "        features = tf.reshape(features, [28])\n",
    "        label = example['label']\n",
    "        return features, label\n",
    "    return decoder\n",
    "\n",
    "def load_dataset(filenames, decoder, ordered=False):\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False\n",
    "    dataset = (\n",
    "        tf.data\n",
    "        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(ignore_order)\n",
    "        .map(decoder, AUTO)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbd856c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = int(11e6)\n",
    "validation_size = int(5e5)\n",
    "training_size = dataset_size - validation_size\n",
    "\n",
    "# For model.fit\n",
    "batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "steps_per_epoch = training_size // batch_size\n",
    "validation_steps = validation_size // batch_size\n",
    "\n",
    "#  For model.compile\n",
    "steps_per_execution = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "745b2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'features': FixedLenFeature([], tf.string),\n",
    "    'label': FixedLenFeature([], tf.float32)\n",
    "}\n",
    "decoder = make_decoder(feature_description)\n",
    "\n",
    "data_dir = 'higgs-boson'\n",
    "train_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\n",
    "valid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n",
    "\n",
    "ds_train = load_dataset(train_files, decoder, ordered=False)\n",
    "ds_train = (\n",
    "    ds_train\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .shuffle(2 ** 19)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "ds_valid = load_dataset(valid_files, decoder, ordered=False)\n",
    "ds_valid = (\n",
    "    ds_valid\n",
    "    .batch(batch_size)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34e00521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The argument `steps_per_execution` is no longer experimental. Pass `steps_per_execution` instead of `experimental_steps_per_execution`.\n"
     ]
    }
   ],
   "source": [
    "def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n",
    "    def make(inputs):\n",
    "        x = layers.Dense(units)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        return x\n",
    "    return make\n",
    "\n",
    "with strategy.scope():\n",
    "    # Wide Network\n",
    "    wide = keras.experimental.LinearModel()\n",
    "\n",
    "    # Deep Network\n",
    "    inputs = keras.Input(shape=[28])\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    deep = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Wide and Deep Network\n",
    "    wide_and_deep = keras.experimental.WideDeepModel(\n",
    "        linear_model=wide,\n",
    "        dnn_model=deep,\n",
    "        activation='sigmoid',\n",
    "    )\n",
    "\n",
    "wide_and_deep.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['AUC', 'binary_accuracy'],\n",
    "    experimental_steps_per_execution=steps_per_execution,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1649fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    patience = 2,\n",
    "    min_delta = 0.001,\n",
    "    restore_best_weights = True,\n",
    ")\n",
    "\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(\n",
    "    patience = 0,\n",
    "    factor = 0.2,\n",
    "    min_lr = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4940edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 512/5126 [=>............................] - ETA: 1:48:31 - loss: 0.6119 - auc: 0.7408 - binary_accuracy: 0.6775"
     ]
    }
   ],
   "source": [
    "history = wide_and_deep.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, lr_schedule],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a49ec2c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_frame \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[0;32m      2\u001b[0m history_frame\u001b[38;5;241m.\u001b[39mloc[:, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mplot(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross-entropy Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m history_frame\u001b[38;5;241m.\u001b[39mloc[:, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mplot(title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n",
    "history_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
